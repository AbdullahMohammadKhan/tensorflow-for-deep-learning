{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100\n",
    "\n",
    "t_data = (mnist.train.images - (255/2.0))/255 \n",
    "train_data = t_data.reshape(55000, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "e_data = (mnist.test.images - (255/2.0))/255\n",
    "eval_data = e_data.reshape(10000, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS).astype(np.float32)\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...].astype(np.float32)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...].astype(np.float32)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_node = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int32, shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder(tf.float32, shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "\n",
    "conv1_weights = tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32], stddev=0.1, seed=SEED, dtype=tf.float32))\n",
    "conv1_biases = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
    "\n",
    "conv2_weights = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1, seed=SEED, dtype=tf.float32))\n",
    "conv2_biases = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
    "\n",
    "fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512], stddev=0.1, seed=SEED, dtype=tf.float32))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=tf.float32))\n",
    "\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS], stddev=0.1, seed=SEED, dtype=tf.float32))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "def model(data, train=False):\n",
    "    conv1 = tf.nn.conv2d(train_data_node, conv1_weights, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2,[pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_labels_node, logits=logits))\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights)\n",
    "                + tf.nn.l2_loss(fc1_biases)\n",
    "                + tf.nn.l2_loss(fc2_weights)\n",
    "                + tf.nn.l2_loss(fc2_biases))\n",
    "\n",
    "loss += 5e-4 * regularizers\n",
    "batch = tf.Variable(0, dtype=tf.float32)\n",
    "learning_rate = tf.train.exponential_decay(0.01, batch * BATCH_SIZE, train_size,0.95,staircase=True)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss,global_step=batch)\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00), 2.5 ms\n",
      "Minibatch loss: 7.012, learning rate: 0.010000\n",
      "Minibatch error: 96.9%\n",
      "Step 100 (epoch 0.13), 90.6 ms\n",
      "Minibatch loss: 5.410, learning rate: 0.010000\n",
      "Minibatch error: 87.5%\n",
      "Step 200 (epoch 0.26), 177.9 ms\n",
      "Minibatch loss: 5.378, learning rate: 0.010000\n",
      "Minibatch error: 89.1%\n",
      "Step 300 (epoch 0.38), 273.9 ms\n",
      "Minibatch loss: 5.340, learning rate: 0.010000\n",
      "Minibatch error: 87.5%\n",
      "Step 400 (epoch 0.51), 366.9 ms\n",
      "Minibatch loss: 5.328, learning rate: 0.010000\n",
      "Minibatch error: 96.9%\n",
      "Step 500 (epoch 0.64), 460.4 ms\n",
      "Minibatch loss: 5.280, learning rate: 0.010000\n",
      "Minibatch error: 98.4%\n",
      "Step 600 (epoch 0.77), 555.1 ms\n",
      "Minibatch loss: 5.229, learning rate: 0.010000\n",
      "Minibatch error: 90.6%\n",
      "Step 700 (epoch 0.90), 647.7 ms\n",
      "Minibatch loss: 5.220, learning rate: 0.010000\n",
      "Minibatch error: 90.6%\n",
      "Step 800 (epoch 1.02), 744.8 ms\n",
      "Minibatch loss: 5.174, learning rate: 0.009500\n",
      "Minibatch error: 89.1%\n",
      "Step 900 (epoch 1.15), 834.7 ms\n",
      "Minibatch loss: 5.134, learning rate: 0.009500\n",
      "Minibatch error: 81.2%\n",
      "Step 1000 (epoch 1.28), 922.8 ms\n",
      "Minibatch loss: 5.139, learning rate: 0.009500\n",
      "Minibatch error: 98.4%\n",
      "Step 1100 (epoch 1.41), 1018.3 ms\n",
      "Minibatch loss: 5.098, learning rate: 0.009500\n",
      "Minibatch error: 85.9%\n",
      "Step 1200 (epoch 1.54), 1112.9 ms\n",
      "Minibatch loss: 5.061, learning rate: 0.009500\n",
      "Minibatch error: 87.5%\n",
      "Step 1300 (epoch 1.66), 1203.8 ms\n",
      "Minibatch loss: 5.040, learning rate: 0.009500\n",
      "Minibatch error: 87.5%\n",
      "Step 1400 (epoch 1.79), 1292.1 ms\n",
      "Minibatch loss: 5.032, learning rate: 0.009500\n",
      "Minibatch error: 90.6%\n",
      "Step 1500 (epoch 1.92), 1388.8 ms\n",
      "Minibatch loss: 4.995, learning rate: 0.009500\n",
      "Minibatch error: 92.2%\n",
      "Step 1600 (epoch 2.05), 1479.1 ms\n",
      "Minibatch loss: 4.974, learning rate: 0.009025\n",
      "Minibatch error: 90.6%\n",
      "Step 1700 (epoch 2.18), 1568.7 ms\n",
      "Minibatch loss: 4.932, learning rate: 0.009025\n",
      "Minibatch error: 89.1%\n",
      "Step 1800 (epoch 2.30), 1655.9 ms\n",
      "Minibatch loss: 4.932, learning rate: 0.009025\n",
      "Minibatch error: 92.2%\n",
      "Step 1900 (epoch 2.43), 1747.5 ms\n",
      "Minibatch loss: 4.898, learning rate: 0.009025\n",
      "Minibatch error: 89.1%\n",
      "Step 2000 (epoch 2.56), 1838.1 ms\n",
      "Minibatch loss: 4.871, learning rate: 0.009025\n",
      "Minibatch error: 87.5%\n",
      "Step 2100 (epoch 2.69), 1932.2 ms\n",
      "Minibatch loss: 4.847, learning rate: 0.009025\n",
      "Minibatch error: 90.6%\n",
      "Step 2200 (epoch 2.82), 2021.3 ms\n",
      "Minibatch loss: 4.814, learning rate: 0.009025\n",
      "Minibatch error: 87.5%\n",
      "Step 2300 (epoch 2.94), 2108.7 ms\n",
      "Minibatch loss: 4.795, learning rate: 0.009025\n",
      "Minibatch error: 82.8%\n",
      "Step 2400 (epoch 3.07), 2203.1 ms\n",
      "Minibatch loss: 4.799, learning rate: 0.008574\n",
      "Minibatch error: 95.3%\n",
      "Step 2500 (epoch 3.20), 2291.7 ms\n",
      "Minibatch loss: 4.771, learning rate: 0.008574\n",
      "Minibatch error: 92.2%\n",
      "Step 2600 (epoch 3.33), 2380.4 ms\n",
      "Minibatch loss: 4.747, learning rate: 0.008574\n",
      "Minibatch error: 93.8%\n",
      "Step 2700 (epoch 3.46), 2469.6 ms\n",
      "Minibatch loss: 4.730, learning rate: 0.008574\n",
      "Minibatch error: 90.6%\n",
      "Step 2800 (epoch 3.58), 2559.2 ms\n",
      "Minibatch loss: 4.702, learning rate: 0.008574\n",
      "Minibatch error: 84.4%\n",
      "Step 2900 (epoch 3.71), 2648.2 ms\n",
      "Minibatch loss: 4.679, learning rate: 0.008574\n",
      "Minibatch error: 89.1%\n",
      "Step 3000 (epoch 3.84), 2736.9 ms\n",
      "Minibatch loss: 4.659, learning rate: 0.008574\n",
      "Minibatch error: 89.1%\n",
      "Step 3100 (epoch 3.97), 2825.5 ms\n",
      "Minibatch loss: 4.645, learning rate: 0.008574\n",
      "Minibatch error: 90.6%\n",
      "Step 3200 (epoch 4.10), 2916.3 ms\n",
      "Minibatch loss: 4.617, learning rate: 0.008145\n",
      "Minibatch error: 89.1%\n",
      "Step 3300 (epoch 4.22), 3004.9 ms\n",
      "Minibatch loss: 4.594, learning rate: 0.008145\n",
      "Minibatch error: 89.1%\n",
      "Step 3400 (epoch 4.35), 3094.3 ms\n",
      "Minibatch loss: 4.581, learning rate: 0.008145\n",
      "Minibatch error: 87.5%\n",
      "Step 3500 (epoch 4.48), 3184.2 ms\n",
      "Minibatch loss: 4.562, learning rate: 0.008145\n",
      "Minibatch error: 89.1%\n",
      "Step 3600 (epoch 4.61), 3273.2 ms\n",
      "Minibatch loss: 4.544, learning rate: 0.008145\n",
      "Minibatch error: 85.9%\n",
      "Step 3700 (epoch 4.74), 3361.4 ms\n",
      "Minibatch loss: 4.535, learning rate: 0.008145\n",
      "Minibatch error: 90.6%\n",
      "Step 3800 (epoch 4.86), 3450.0 ms\n",
      "Minibatch loss: 4.510, learning rate: 0.008145\n",
      "Minibatch error: 90.6%\n",
      "Step 3900 (epoch 4.99), 3539.4 ms\n",
      "Minibatch loss: 4.496, learning rate: 0.008145\n",
      "Minibatch error: 95.3%\n",
      "Step 4000 (epoch 5.12), 3628.1 ms\n",
      "Minibatch loss: 4.466, learning rate: 0.007738\n",
      "Minibatch error: 87.5%\n",
      "Step 4100 (epoch 5.25), 3716.8 ms\n",
      "Minibatch loss: 4.461, learning rate: 0.007738\n",
      "Minibatch error: 93.8%\n",
      "Step 4200 (epoch 5.38), 3805.6 ms\n",
      "Minibatch loss: 4.448, learning rate: 0.007738\n",
      "Minibatch error: 93.8%\n",
      "Step 4300 (epoch 5.50), 3893.9 ms\n",
      "Minibatch loss: 4.423, learning rate: 0.007738\n",
      "Minibatch error: 90.6%\n",
      "Step 4400 (epoch 5.63), 3982.1 ms\n",
      "Minibatch loss: 4.415, learning rate: 0.007738\n",
      "Minibatch error: 87.5%\n",
      "Step 4500 (epoch 5.76), 4070.8 ms\n",
      "Minibatch loss: 4.395, learning rate: 0.007738\n",
      "Minibatch error: 89.1%\n",
      "Step 4600 (epoch 5.89), 4159.7 ms\n",
      "Minibatch loss: 4.375, learning rate: 0.007738\n",
      "Minibatch error: 92.2%\n",
      "Step 4700 (epoch 6.02), 4248.6 ms\n",
      "Minibatch loss: 4.352, learning rate: 0.007351\n",
      "Minibatch error: 89.1%\n",
      "Step 4800 (epoch 6.14), 4337.1 ms\n",
      "Minibatch loss: 4.340, learning rate: 0.007351\n",
      "Minibatch error: 84.4%\n",
      "Step 4900 (epoch 6.27), 4426.4 ms\n",
      "Minibatch loss: 4.332, learning rate: 0.007351\n",
      "Minibatch error: 92.2%\n",
      "Step 5000 (epoch 6.40), 4515.0 ms\n",
      "Minibatch loss: 4.315, learning rate: 0.007351\n",
      "Minibatch error: 89.1%\n",
      "Step 5100 (epoch 6.53), 4603.7 ms\n",
      "Minibatch loss: 4.299, learning rate: 0.007351\n",
      "Minibatch error: 89.1%\n",
      "Step 5200 (epoch 6.66), 4692.0 ms\n",
      "Minibatch loss: 4.297, learning rate: 0.007351\n",
      "Minibatch error: 92.2%\n",
      "Step 5300 (epoch 6.78), 4780.2 ms\n",
      "Minibatch loss: 4.260, learning rate: 0.007351\n",
      "Minibatch error: 84.4%\n",
      "Step 5400 (epoch 6.91), 4868.7 ms\n",
      "Minibatch loss: 4.256, learning rate: 0.007351\n",
      "Minibatch error: 85.9%\n",
      "Step 5500 (epoch 7.04), 4957.2 ms\n",
      "Minibatch loss: 4.245, learning rate: 0.006983\n",
      "Minibatch error: 89.1%\n",
      "Step 5600 (epoch 7.17), 5046.4 ms\n",
      "Minibatch loss: 4.229, learning rate: 0.006983\n",
      "Minibatch error: 87.5%\n",
      "Step 5700 (epoch 7.30), 5134.8 ms\n",
      "Minibatch loss: 4.206, learning rate: 0.006983\n",
      "Minibatch error: 85.9%\n",
      "Step 5800 (epoch 7.42), 5223.6 ms\n",
      "Minibatch loss: 4.209, learning rate: 0.006983\n",
      "Minibatch error: 92.2%\n",
      "Step 5900 (epoch 7.55), 5313.0 ms\n",
      "Minibatch loss: 4.191, learning rate: 0.006983\n",
      "Minibatch error: 90.6%\n",
      "Step 6000 (epoch 7.68), 5401.8 ms\n",
      "Minibatch loss: 4.186, learning rate: 0.006983\n",
      "Minibatch error: 90.6%\n",
      "Step 6100 (epoch 7.81), 5491.4 ms\n",
      "Minibatch loss: 4.163, learning rate: 0.006983\n",
      "Minibatch error: 90.6%\n",
      "Step 6200 (epoch 7.94), 5580.3 ms\n",
      "Minibatch loss: 4.152, learning rate: 0.006983\n",
      "Minibatch error: 89.1%\n",
      "Step 6300 (epoch 8.06), 5669.4 ms\n",
      "Minibatch loss: 4.136, learning rate: 0.006634\n",
      "Minibatch error: 93.8%\n",
      "Step 6400 (epoch 8.19), 5758.5 ms\n",
      "Minibatch loss: 4.130, learning rate: 0.006634\n",
      "Minibatch error: 90.6%\n",
      "Step 6500 (epoch 8.32), 5847.1 ms\n",
      "Minibatch loss: 4.113, learning rate: 0.006634\n",
      "Minibatch error: 89.1%\n",
      "Step 6600 (epoch 8.45), 5936.3 ms\n",
      "Minibatch loss: 4.104, learning rate: 0.006634\n",
      "Minibatch error: 92.2%\n",
      "Step 6700 (epoch 8.58), 6025.3 ms\n",
      "Minibatch loss: 4.084, learning rate: 0.006634\n",
      "Minibatch error: 90.6%\n",
      "Step 6800 (epoch 8.70), 6114.3 ms\n",
      "Minibatch loss: 4.081, learning rate: 0.006634\n",
      "Minibatch error: 90.6%\n",
      "Step 6900 (epoch 8.83), 6203.1 ms\n",
      "Minibatch loss: 4.064, learning rate: 0.006634\n",
      "Minibatch error: 87.5%\n",
      "Step 7000 (epoch 8.96), 6291.9 ms\n",
      "Minibatch loss: 4.054, learning rate: 0.006634\n",
      "Minibatch error: 87.5%\n",
      "Step 7100 (epoch 9.09), 6380.8 ms\n",
      "Minibatch loss: 4.034, learning rate: 0.006302\n",
      "Minibatch error: 82.8%\n",
      "Step 7200 (epoch 9.22), 6469.9 ms\n",
      "Minibatch loss: 4.023, learning rate: 0.006302\n",
      "Minibatch error: 84.4%\n",
      "Step 7300 (epoch 9.34), 6559.0 ms\n",
      "Minibatch loss: 4.014, learning rate: 0.006302\n",
      "Minibatch error: 85.9%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "    return 100.0 - (\n",
    "      100.0 *\n",
    "      np.sum(np.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0])\n",
    "\n",
    "def eval_in_batches(data, sess):\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.ndarray(shape=(size, NUM_LABELS),dtype=np.float32)\n",
    "    for begin in range(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            predictions[begin:end, :] = sess.run(eval_prediction,feed_dict={eval_data: data[begin:end, ...]})\n",
    "        else:\n",
    "            batch_predictions = sess.run(\n",
    "            eval_prediction, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE)]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            l, lr, predictions = sess.run([loss, learning_rate, train_prediction],feed_dict=feed_dict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Step %d (epoch %.2f), %.1f ms' % (step, float(step) * BATCH_SIZE / train_size, 1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "            print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "            print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "#             print('Validation error: %.1f%%' % error_rate(eval_in_batches(validation_data, sess), validation_labels))\n",
    "    \n",
    "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
